\documentclass{csse4400}

\usepackage{languages}

% RUBRIC
\usepackage{multirow}
\usepackage{array}
\usepackage{xltabular}
\usepackage{pdflscape}
\usepackage{enumitem}

\newcolumntype{P}[1]{>{\centering\arraybackslash}p{#1}}
% RUBRIC

\title{SpamOverflow}
\author{Evan Hughes}
\date{Semester 1, 2024}

\begin{document}
\maketitle

\section*{Summary}
In this assignment, you will demonstrate your ability to \textsl{design},
\textsl{implement}, and \textsl{deploy} a web API that can process a high load,
i.e. a scalable application.
You are to deloy an API for scanning and filtering spam/malicious emails.
Specially your application needs to support:
\begin{itemize}
    \item Scanning an email via an API request.
    \item Providing access via a specified REST API, e.g. for use by front-end interfaces.
    \item Remaining responsive to the user while scanning emails.
\end{itemize}

Your service will be deployed to AWS and will undergo automated correctness and load-testing to ensure it meets the requirements.

\section{Introduction}
....

\paragraph{Task}
For this assignment, you are working for SpamOverflow, a new competitor in the email security space. SpamOverflow uses a microservices based architecture to implement their new platform. The CEO saw on your resume that you took Software Architecture and has assigned you to design and implement the service. This service must be scalable to cope with a large influx of emails that occur over the day.

\paragraph{Requirements}
Some email filtering software can be in the flow of traffic or it can be used to scan emails that have already arrived. SpamOverflow has decided to implement a service that does not impeed the flow of traffic and gets a API call at reciept of an email. The service then pulls the email from the users inbox as fast as it can to stop the user from seeing the email or clicking any links.

Mail providers like Microsoft and Google only send a single API request for each email recieved, so for optional performance this service needs to be able to handle a large number of requests in a short period of time to not miss any emails.

Since these emails can be dangerous, the service must be able to report that it is bad or good in a timely manner. Though genuine emails that are incorrectly marked as dangerous should be returned to the user as quickly as possible.

Persistence is an important characteristic of the platform. Customers will report to do reports and analyse why emails were flagged and naturally will be upset if the system loses it because a server crashed. Upon receiving a email scan request, the system must guarantee that the data has been saved to persistent storage before returning a success response.

\section{Interface}
As you are operating in a microservices context, other service providers have been given an API specification for your service. They have been developing their services based on this specification so you must match it exactly.

The interface specification is available to all service owners online: \url{https://csse6400.uqcloud.net/assessment/spamoverflow}

\section{Implementation}
The following constraints apply to the implementation of your assignment solution.

\subsection{SpamHammer}

A collection of scanners have been provided to you by the company's security team. A suite of these tools are developed by Mr Hughes which use common metrics to know if an emails bad but arnt always accurate. One of these tools is developed by Dr Richardson who is an Ai and Lingustic expert and is 100 percent accurate but is very slow. Unfortunately because the company wants to aim for a low false positive rate the CEO wants both accuracy and speed. You will have to work around this bottleneck in the design and development of your parts of the system.

\warning{You are not allowed to reimplement or modify this tool.}

Your service must utilise the \texttt{spamhammer} command line tool provided for this assignment. You may not make any modifications to this tool. The compiled binaries are available in the tool's GitHub repository: \url{https://github.com/CSSE6400/spamhammer}

This tool is not as magical as it sounds, in the API you will notice a field that must be included that makes it all the way to the scanner but isnt part of the email text. This is a setting which decides what each scanner is going to return ( either a 0 or 1 ) and contains the info if the email is actually good or bad. Demos are provided in the repository to show how to use the tool to generate your own examples.

\subsection{Similarity}

Dr Richardson has also provided some advice to help you with filtering through the emails and has suggested that you use a similarity metric to compare the emails to a database of known bad emails. The Dr explains it as "many of the emails we have seen with our first customers have the same content and structure its just that the link or 'Dear <name>' is slightly different".

With this knowledge you have found a common method of getting the difference between documents called the Cosine Similarity which is explained in some videos you found on youtube.

\begin{itemize}
  \item \url{https://www.youtube.com/watch?v=e9U0QAFbfLI}
  \item \url{https://www.youtube.com/watch?v=Dd16LVt5ct4}
\end{itemize}

\info{Dr Richardson emphasies that the similarity metric is not a replacement for the scanners and is optional way to improve the effiency of the system.}

\subsection{AWS Services}
Please make note of the \link{AWS services}
{https://labs.vocareum.com/web/2460291/1564816.0/ASNLIB/public/docs/lang/en-us/README.html\#services}
that you can use in the AWS Learner Labs, and the limitations that are placed on the usage of these services.
To view this page you need to be logged in to your AWS learner lab environment and have a lab open.

\subsection{External Services}
You may \textbf{not} use services or products from outside of the AWS Learner Labs environment.
For example, you may not host instances of the \texttt{spamhammer} command line tool on another cloud platform
(e.g. Google Cloud).

You may \textbf{not} use services or products that run on AWS infrastructure external to your learner lab environment.
For example, you may not deploy a third-party product like MongoDB Atlas on AWS and then use it from your service.

You may \textbf{not} deploy machine learning or GPU backed services.

\section{Submission}
Your solution must committed to the GitHub repository specified in Section \ref{sec:github}.
Committing to this repository will be disabled after \textbf{16:00 (AEST) on 3rd May 2023} and the contents of the repository at this time will be taken as your submission.
The repository \textbf{must} contain everything required to successfully deploy your application.
You must include all of the following in the repository:
\begin{itemize}
  \item Your implementation of the service API, including the source code and a mechanism to build the service.%
  \footnote{If you use external libraries, ensure that you pin the versions to avoid external changes breaking your application.}
  \item Terraform code that can provision your service in a fresh AWS Learner Lab environment.
  \item A \texttt{deploy.sh} script that can use your Terraform code to deploy your application.
    This script can perform other tasks as required.
\end{itemize}

When deploying your application to mark, we will follow reproducible steps, outlined below. You may re-create the process yourself.

\begin{enumerate}
  \item Your Git repository will be checked out locally.
  \item AWS credentials will be copied into your repository in the top-level directory,
  in a file called \texttt{credentials}.
  \item The script \texttt{deploy.sh} in the top-level of the repository will be run.
  \item The \texttt{deploy.sh} script \textbf{must} create a file named \texttt{api.txt} which contains the URL at which your API is deployed, e.g. \texttt{http://my-lb.com/}
  \item We will run automated functionality and load-testing on the URL provided in the \texttt{api.txt} file.
\end{enumerate}

\textbf{Important Note: Ensure your service does not exceed the resource limits of AWS Learner Labs. For example, AWS will deactivate your account if more than 15 EC2 instances are running.}

\subsection{GitHub Repository}\label{sec:github}
You will be provisioned with a private repo on GitHub for this assignment, via GitHub Classroom. You must click on the link below and associate your GitHub username with your UQ student ID in the Classroom.

\url{https://classroom.github.com/a/ZW_8y-7G}

\noindent
Associating your GitHub username with another student's ID, or getting someone else to associate their GitHub username with your student ID, is \link{academic misconduct} {https://my.uq.edu.au/information-and-services/manage-my-program/student-integrity-and-conduct/academic-integrity-and-student-conduct}.

If for some reason you have accidently associated your GitHub username with the wrong student ID, contact the course staff as soon as possible.

\subsection{Tips}

\paragraph{If something goes wrong}
Ideally, your infrastructure will be deployed successfully but this could go wrong for any number of unforeseen reasons. If your deployment is automatically detected to have an issue then a redeployment will be attempted. If this fails then it will be up to the discresion of the course coordinator on how to proceed.

The assessment is split into multiple checkpoints to help ensure that you are on the right track and that your deployment functions.

\paragraph{Terraform plan/apply hanging}
If your \texttt{terraform plan} or \texttt{terraform apply} command hangs without any output, check your AWS credentials. Using credentials of an expired learner lab session will cause Terraform to hang.

\paragraph{Fresh AWS learner lab}
Your AWS learner lab can be reset using the reset button in the learner lab toolbar. \includegraphics[width=\textwidth]{images/reset-button.png} To ensure that you are not accidentally depending on anything specific to your learner lab environment, we recommend that you reset your lab prior to final submission. Note that resetting the lab can take a considerable amount of time, in the order of hours. You should do this at least 4 to 6 hours before the submission deadline. Please do not wait to the last minute.

\paragraph{Deploying with Docker}
In this course, you have been shown how to use Docker containers to deploy on ECS. You may refer to the practical worksheets for a description of how to deploy with containers \cite{prac-week5}.

\subsection{Fine Print}
You can reproduce our process for deploying your application using our \link{Docker image}{https://ghcr.io/CSSE6400/csse6400-cloud-testing}:
\codefile[language=docker]{Dockerfile}{deployment/Dockerfile}

Our steps for deploying your infrastructure using this container are as follows.
\texttt{\$REPO} is the name of your repository, and
\texttt{\$CREDENTIALS} is the path where we will store our AWS credentials.
\begin{code}[language=shell]{}
$ git clone git@github.com:CSSE6400/$REPO
$ cp $CREDENTIALS $REPO
$ docker run -v /var/run/docker.sock:/var/run/docker.sock -v $(pwd)/$REPO:/workspace csse6400-cloud-testing
$ cat $REPO/api.txt # this will be used for load-testing
\end{code}

\noindent
Note that the Docker socket of the host has been mounted. This enables running \texttt{docker} in the container. This has been tested on MacOSX and Linux but may require WSL2 on Windows.


\section{Criteria}
Your assignment submission will be assessed on its ability to support the specified use cases. Testing is divided into functionality testing, deployment and quality testing. Functionality testing is to ensure that your backend software and API meet the MVP requirements by satisfying the API specification without any excessive load. Deployment is to ensure that this MVP can then be hosted in the target cloud provider. Quality testing is based upon several likely use case scenarios. The scenarios create different scaling requirements.

Partial marks are available for both types of tests, i.e. if some functionality is implemented you can receive marks for that functionality, or if your service can handle 80\% of the scenario during quality testing you will receive marks for that. Deployment is marked in its entirety.

\subsection{Functionality}
40\% of the total marks for the assignment are for correctly implementing the API specification, irrespective of whether it is able to cope with high loads. A suite of automated API tests will assess the correctness of your implementation, via a sequence of API calls. The API test suite will be made available before the functionality due date.

\subsection{Deployment}
25\% of the total marks for the assignment are for deploying a correctly implemented service to AWS irrespective of whether it is able to cope with high loads. The deployment will be assessed by running a script that deploys your service to AWS that then runs a suite of automated API tests to access the correctness of your implementation. The script will be provided to you before the deployment checkpoint due date with the API test suite.

\subsection{Quality Scenarios}\label{sec:scenarios}
The remaining 35\% of the marks will be derived from how well your service handles various scenarios. These scenarios will require you to consider how your application performs under load. Examples of possible scenarios are described below. These are not descriptions of specific tests that will be run, rather they are examples of the types of tests that will be run.


\paragraph{...}
.

\paragraph{...}
.

\paragraph{...}
.

\paragraph{Leaked directory}
A bad actor has managed to get the email addresses of all employees of <Large Company>. They have sent a phishing email to all of the users advertising a pay raise with a link to a fake login page. The email is sent to all 10,000 employees at the same time.

\paragraph{...}
.

\subsection{Marking}
Functionality accounts for 40\% of the marks for the assignment. This is split as 25\% for correct implementation of the provided API, and 15\% for correct generation of tickets and seating plans. The simple queries in the API are worth much less of the mark compared to the API operations that require processing of data.

Functionality marks are based on correct implementation of the functionality, which is primarily assessed by the automated functionality tests.

Persistence is a core functional requirement of the system. If your implementation does not save all new email scans to persistent storage, your grade for the assignment will be capped at 4.

Your persistence mechanism must be robust, so that it can cope with catastrophic failure of the system. If all running instances of your services are terminated, the system must be able to restart and guarantee that it has not lost any data about emails for which it returned a success response to the caller. There will not be a test that explicitly kills all services and restarts the system. This will be assessed based on the services you use and how your implementation invokes those services. If you do not store data to a persistent data store, or you return a success response before the data has been saved, are the criteria that determine whether you have successfully implemented persistence.

Deploying your service is worth 25\% of the marks for the assignment. This is based on the success deployment using terraform of your service to AWS and the ability to access the service via the API. Your service must be fully functional while deployed so the functionality tests can be run which determines the marks for deployment.

Scaling your application to deliver the quality scenarios accounts for the other 35\% of the marks. The scenarios described in section \ref{sec:scenarios} provide guidance as to the type of scalability issues your system is expected to handle. They are not literal descriptions of the exact loads that will be used. Tests related to scenarios that involve more complex behaviour will have higher weight than other tests.

The scenarios will also evaluate whether your service is being wasteful in resource usage. The amount of resources deployed in your AWS account will be monitored to ensure that your service implements a scaling up and scaling down procedure.

All stages of the assessment will be marked using automation and a subset of the tests will be released. These tests may consume a \textbf{\emph{significant}} portion of your AWS credit. You are advised to be prudent in how many times you execute these tests. The tests for Functionality and Deployment will be released in their entirety for the checkpoints but the tests for Quality Scenarios will only be released at the Course Coordinators discression or after the due date.

Please refer to the marking criteria at the end of this document.

\section{Academic Integrity}
As this is a higher-level course, you are expected to be familiar with the importance of academic integrity in general, and the details of UQ's rules. If you need a reminder, review the \link{Academic Integrity Modules} {https://web.library.uq.edu.au/library-services/it/learnuq-blackboard-help/academic-integrity-modules}. Submissions will be checked to ensure that the work submitted is not plagiarised.

This is an individual assignment. You may not discuss details of approaches to solve the problem with other students in the course. All code that you submit must be your own work. You may not directly copy code that you have found online to solve parts of the assignment. If you find ideas from online sources (e.g. Stack Overflow), you must \link{cite and reference}{https://web.library.uq.edu.au/node/4221/2} these sources. Use the \link{IEEE referencing style}{https://libraryguides.vu.edu.au/ieeereferencing/gettingstarted} for citations and references. Citations should be included in a comment at the location where the idea is used in your code. All references for citations must be included in a file called \texttt{refs.txt}. This file should be in the root directory of your project.

Uncited or unreferenced material will be treated as not being your own work. Significant amounts of cited material from other sources will be considered to be of no academic merit.


\bibliographystyle{ieeetr}
\bibliography{ours}

\input{criteria}

\end{document}


